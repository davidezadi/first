{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidezadi/first/blob/master/Copy_of_ultimate_Gemini_srt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title The Definitive Splitting & Patching Pipeline - V9.7 (Sanitized Filenames)\n",
        "\n",
        "# @markdown ### ‚öôÔ∏è Step 1: Intelligent Dependency Installation\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    from tqdm import tqdm\n",
        "    from IPython.display import display, HTML\n",
        "    print(\"‚úÖ Required libraries already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required libraries (tqdm, google-generativeai, ipython)...\")\n",
        "    !pip install -q google-generativeai tqdm ipython\n",
        "    import google.generativeai as genai\n",
        "    from tqdm import tqdm\n",
        "    from IPython.display import display, HTML\n",
        "    print(\"‚úÖ Libraries installed successfully.\")\n",
        "\n",
        "# --- Import necessary libraries ---\n",
        "from google.colab import drive\n",
        "import subprocess, os, shutil, json, time, sys, logging, re\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import tempfile\n",
        "from typing import List, NamedTuple, Optional, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import concurrent.futures\n",
        "import base64\n",
        "import random\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üìÇ Step 2: Define Your Workspace & Filenames\n",
        "WORKSPACE_PATH = \"/content/drive/MyDrive/GeminiProcessor\"  # @param {type:\"string\"}\n",
        "KEYS_FILENAME = \"keys.json\"  # @param {type:\"string\"}\n",
        "PROMPT_FILENAME = \"prompt.txt\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîß Step 3: Set Processing Options\n",
        "MODEL_NAME = \"gemini-2.5-pro\" # @param {type:\"string\"}\n",
        "FORCE_REPROCESS = False #@param {type:\"boolean\"}\n",
        "CHUNK_DURATION_MINUTES = 9.9999 #@param {type:\"number\"}\n",
        "OUTPUT_FILENAME_SUFFIX = \"_result\" #@param {type:\"string\"}\n",
        "# @markdown **PERFORMANCE SETTINGS:**\n",
        "# @markdown Number of parallel processes. Set this to the number of API keys for maximum speed.\n",
        "NUM_WORKERS = 10  # @param {type:\"integer\"}\n",
        "# @markdown Number of times to retry a failed chunk before giving up.\n",
        "MAX_RETRIES = 2  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### ‚ö° Step 4: Prevent Runtime Disconnects\n",
        "ENABLE_KEEP_ALIVE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# --- 1. CORE DATA STRUCTURES & WORKER SCRIPT ---\n",
        "\n",
        "WORKER_SCRIPT_TEMPLATE = \"\"\"\n",
        "import google.generativeai as genai\n",
        "import sys, time, base64, json, random\n",
        "from pathlib import Path\n",
        "\n",
        "def run():\n",
        "    api_keys_b64 = sys.argv[1]\n",
        "    model_name = sys.argv[2]\n",
        "    prompt_b64 = sys.argv[3]\n",
        "    audio_chunk_path_str = sys.argv[4]\n",
        "    output_path_str = sys.argv[5]\n",
        "\n",
        "    try:\n",
        "        api_keys_json = base64.b64decode(api_keys_b64).decode('utf-8')\n",
        "        api_keys = json.loads(api_keys_json)\n",
        "        random.shuffle(api_keys)\n",
        "    except Exception as e:\n",
        "        print(f\"Worker script failed to decode API keys: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    prompt = base64.b64decode(prompt_b64).decode('utf-8')\n",
        "    audio_chunk_path = Path(audio_chunk_path_str)\n",
        "    output_path = Path(output_path_str)\n",
        "\n",
        "    gemini_file_to_clean = None\n",
        "    last_error = None\n",
        "    success = False\n",
        "\n",
        "    for api_key in api_keys:\n",
        "        try:\n",
        "            genai.configure(api_key=api_key)\n",
        "            model = genai.GenerativeModel(model_name=model_name)\n",
        "\n",
        "            for attempt in range(3):\n",
        "                try:\n",
        "                    uploaded_file = genai.upload_file(path=str(audio_chunk_path))\n",
        "                    gemini_file_to_clean = uploaded_file\n",
        "                    break # Success\n",
        "                except Exception as e:\n",
        "                    if attempt < 2:\n",
        "                        time.sleep(5 * (attempt + 1))\n",
        "                    else:\n",
        "                        raise e # Propagate error to trigger key rotation\n",
        "\n",
        "            response = model.generate_content([prompt, gemini_file_to_clean])\n",
        "\n",
        "            if not response or not hasattr(response, 'text') or not response.text:\n",
        "                raise ValueError(\"API returned an empty or invalid response.\")\n",
        "\n",
        "            output_path.write_text(response.text.strip(), encoding='utf-8')\n",
        "            success = True\n",
        "            break # Exit the key loop on success\n",
        "\n",
        "        except Exception as e:\n",
        "            key_preview = api_key[-4:]\n",
        "            print(f\"Attempt with key '...{key_preview}' failed: {e}\", file=sys.stderr)\n",
        "            last_error = e\n",
        "            if gemini_file_to_clean:\n",
        "                try:\n",
        "                    genai.delete_file(gemini_file_to_clean.name)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                gemini_file_to_clean = None\n",
        "            time.sleep(2)\n",
        "\n",
        "    try:\n",
        "        if success and gemini_file_to_clean:\n",
        "            genai.delete_file(gemini_file_to_clean.name)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if not success:\n",
        "        raise Exception(f\"All API key attempts failed. Last error: {last_error}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        run()\n",
        "    except Exception as e:\n",
        "        print(f\"Worker script failed: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\"\"\"\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    workspace_path: Path; keys_filename: str; prompt_filename: str; output_suffix: str; model_name: str; prompt_template: str\n",
        "    chunk_duration_seconds: float; retry_delay_seconds: int = 5\n",
        "    @property\n",
        "    def input_path(self) -> Path: return self.workspace_path / \"input\"\n",
        "    @property\n",
        "    def results_path(self) -> Path: return self.workspace_path / \"results\"\n",
        "    @property\n",
        "    def temp_chunk_path(self) -> Path: return self.workspace_path / \"results\" / \"temp_chunks\"\n",
        "    @property\n",
        "    def logs_path(self) -> Path: return self.workspace_path / \"logs\"\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ProcessTask:\n",
        "    media_path: Path; original_stem: str; chunk_index: int; time_offset_seconds: float\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    duration: float; expected_chunks: int\n",
        "\n",
        "class ValidatedSetup(NamedTuple):\n",
        "    config: Config; api_keys: List[str]; log_file_path: Path\n",
        "\n",
        "# --- NEW HELPER FUNCTION TO SANITIZE FILENAMES ---\n",
        "def sanitize_stem(filename_stem: str) -> str:\n",
        "    \"\"\"Creates a filesystem-safe version of a filename stem.\"\"\"\n",
        "    s = re.sub(r'[^\\w\\s-]', '', filename_stem)\n",
        "    s = re.sub(r'[\\s-]+', '-', s).strip('-')\n",
        "    return s[:150]\n",
        "\n",
        "def try_patch_file(stem: str, config: Config, file_info_cache: Dict[str, FileInfo]):\n",
        "    \"\"\"\n",
        "    Checks if a file is ready to be patched. Uses the SANITIZED stem for all lookups.\n",
        "    \"\"\"\n",
        "    final_result_path = config.results_path / f\"{stem}{config.output_suffix}.txt\"\n",
        "    if final_result_path.exists(): return\n",
        "\n",
        "    info = file_info_cache.get(stem)\n",
        "    if not info: return\n",
        "\n",
        "    chunk_files = list(config.temp_chunk_path.glob(f\"{stem}_chunk_*.txt\"))\n",
        "    if len(chunk_files) == info.expected_chunks:\n",
        "        logging.info(f\"‚úÖ All {info.expected_chunks} chunks for '{stem}' are present. Patching, Validating, and Re-indexing now...\")\n",
        "        sorted_chunks = sorted(chunk_files, key=lambda p: int(re.search(r'_chunk_(\\d+)', p.name).group(1)))\n",
        "\n",
        "        TIMESTAMP_RE = re.compile(r\"(?:(\\d{1,2}):)?(\\d{1,2}):(\\d{1,2})[,.](\\d{1,3})\")\n",
        "        final_blocks = []\n",
        "        global_srt_index = 1\n",
        "\n",
        "        for i, chunk_file in enumerate(sorted_chunks):\n",
        "            offset_seconds = i * config.chunk_duration_seconds\n",
        "            chunk_text = chunk_file.read_text(encoding='utf-8')\n",
        "            srt_blocks = re.split(r'\\n\\s*\\n', chunk_text.strip())\n",
        "\n",
        "            for block in srt_blocks:\n",
        "                lines = block.strip().split('\\n')\n",
        "                if len(lines) < 2: continue\n",
        "                time_line = lines[1]\n",
        "                try:\n",
        "                    parts = time_line.split(\"-->\")\n",
        "                    if len(parts) != 2: continue\n",
        "                    start_str, end_str = [s.strip() for s in parts]\n",
        "                    start_match, end_match = TIMESTAMP_RE.match(start_str), TIMESTAMP_RE.match(end_str)\n",
        "                    if not start_match or not end_match: continue\n",
        "\n",
        "                    def calculate_new_time_str(match, offset):\n",
        "                        h_str, m_str, s_str, ms_str = match.groups()\n",
        "                        ms_str = ms_str.ljust(3, '0')\n",
        "                        h = int(h_str) if h_str is not None else 0\n",
        "                        m, s, ms = int(m_str), int(s_str), int(ms_str)\n",
        "                        chunk_total_seconds = (h * 3600) + (m * 60) + s + (ms / 1000.0)\n",
        "                        final_total_seconds = chunk_total_seconds + offset\n",
        "                        if final_total_seconds < 0: final_total_seconds = 0\n",
        "                        new_h, rem = divmod(final_total_seconds, 3600)\n",
        "                        new_m, rem = divmod(rem, 60)\n",
        "                        new_s, new_ms_frac = divmod(rem, 1)\n",
        "                        new_ms = int(round(new_ms_frac * 1000))\n",
        "                        if new_ms >= 1000: new_s += 1; new_ms -= 1000\n",
        "                        if new_s >= 60: new_m +=1; new_s -= 60\n",
        "                        if new_m >= 60: new_h +=1; new_m -= 60\n",
        "                        return f\"{int(new_h):02}:{int(new_m):02}:{int(new_s):02},{new_ms:03}\"\n",
        "\n",
        "                    new_start_str = calculate_new_time_str(start_match, offset_seconds)\n",
        "                    new_end_str = calculate_new_time_str(end_match, offset_seconds)\n",
        "                    new_time_line = f\"{new_start_str} --> {new_end_str}\"\n",
        "                    text_lines = \"\\n\".join(lines[2:])\n",
        "                    valid_block = f\"{global_srt_index}\\n{new_time_line}\\n{text_lines}\"\n",
        "                    final_blocks.append(valid_block)\n",
        "                    global_srt_index += 1\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        final_text = \"\\n\\n\".join(final_blocks)\n",
        "        final_result_path.write_text(final_text, encoding='utf-8')\n",
        "        logging.info(f\"‚úÖ Saved final patched result for '{stem}'. Cleaning up temp chunks.\")\n",
        "        for f in chunk_files:\n",
        "            try: f.unlink()\n",
        "            except OSError as e: logging.warning(f\"Could not delete temp chunk {f.name}: {e}\")\n",
        "\n",
        "def process_chunk_worker(task: ProcessTask, config: Config, api_keys: List[str], file_info_cache: Dict[str, FileInfo]) -> Tuple[str, Optional[ProcessTask]]:\n",
        "    # The task.original_stem is now the SANITIZED stem\n",
        "    log_prefix = f\"[{task.original_stem}_chunk_{task.chunk_index}]\"\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as local_worker_dir_str:\n",
        "            local_worker_dir = Path(local_worker_dir_str)\n",
        "            source_suffix = task.media_path.suffix.lower()\n",
        "            compatible_audio_formats = ['.mp3', '.flac', '.wav', '.m4a', '.aac', '.ogg']\n",
        "            audio_codec = ['-c:a', 'copy'] if source_suffix in compatible_audio_formats else ['-q:a', '0']\n",
        "            output_extension = source_suffix if source_suffix in compatible_audio_formats else '.mp3'\n",
        "            local_audio_chunk = local_worker_dir / f\"chunk_audio{output_extension}\"\n",
        "\n",
        "            logging.info(f\"{log_prefix} Extracting audio chunk (Mode: {'Copy' if '-c:a' in audio_codec else 'Encode'})...\")\n",
        "            command = ['ffmpeg', '-y', '-i', str(task.media_path), '-ss', str(task.time_offset_seconds), '-t', str(config.chunk_duration_seconds), '-vn'] + audio_codec + ['-map', 'a', str(local_audio_chunk)]\n",
        "            subprocess.run(command, check=True, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            api_keys_json = json.dumps(api_keys)\n",
        "            api_keys_b64 = base64.b64encode(api_keys_json.encode('utf-8')).decode('utf-8')\n",
        "            prompt_b64 = base64.b64encode(config.prompt_template.encode('utf-8')).decode('utf-8')\n",
        "            output_path = config.temp_chunk_path / f\"{task.original_stem}_chunk_{task.chunk_index}{config.output_suffix}.txt\"\n",
        "            worker_script_path = local_worker_dir / \"worker.py\"\n",
        "            worker_script_path.write_text(WORKER_SCRIPT_TEMPLATE)\n",
        "            worker_command = [sys.executable, str(worker_script_path), api_keys_b64, config.model_name, prompt_b64, str(local_audio_chunk), str(output_path)]\n",
        "            logging.info(f\"{log_prefix} Starting isolated subprocess for API call...\")\n",
        "            result = subprocess.run(worker_command, capture_output=True, text=True)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                logging.info(f\"{log_prefix} ‚úÖ Subprocess finished successfully.\")\n",
        "                try_patch_file(task.original_stem, config, file_info_cache)\n",
        "                return ('success', task)\n",
        "            else:\n",
        "                raise Exception(f\"Subprocess failed with error: {result.stderr.strip()}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{log_prefix} üö® PROCESSING FAILED: {e}\", exc_info=False)\n",
        "        return ('failure', task)\n",
        "\n",
        "def setup_logging(log_file_path: Path):\n",
        "    log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
        "    root_logger = logging.getLogger(); root_logger.setLevel(logging.INFO)\n",
        "    root_logger.handlers.clear()\n",
        "    root_logger.addHandler(logging.FileHandler(log_file_path))\n",
        "    root_logger.addHandler(logging.StreamHandler(sys.stdout))\n",
        "    logging.info(\"Main process logging configured.\")\n",
        "\n",
        "def setup_and_validate(workspace_str, keys_str, prompt_str, chunk_min) -> ValidatedSetup:\n",
        "    print(\"--- Phase 1: Setting up and validating environment ---\")\n",
        "    try: drive.mount('/content/drive', force_remount=True)\n",
        "    except Exception as e: raise Exception(f\"Google Drive mount failed: {e}. If 'Mountpoint... exists', use 'Runtime -> Disconnect and delete'.\")\n",
        "    workspace_path = Path(workspace_str)\n",
        "    logs_path = workspace_path / \"logs\"\n",
        "    logs_path.mkdir(exist_ok=True)\n",
        "    log_file = logs_path / f\"batch_log_{time.strftime('%Y%m%d-%H%M%S')}.log\"\n",
        "    setup_logging(log_file)\n",
        "    for name, path in {'Workspace': workspace_path, 'Keys': workspace_path / keys_str, 'Prompt': workspace_path / prompt_str, 'Input': workspace_path / \"input\"}.items():\n",
        "        if not path.exists(): raise FileNotFoundError(f\"üö® FATAL ERROR: {name} path ('{path}') not found.\")\n",
        "    logging.info(\"‚úÖ Drive mounted and workspace paths verified.\")\n",
        "    try:\n",
        "        api_keys = json.loads((workspace_path / keys_str).read_text(encoding='utf-8'))\n",
        "        if not isinstance(api_keys, list) or not all(isinstance(k, str) and k for k in api_keys): raise ValueError(\"Keys file must be a JSON list of non-empty strings.\")\n",
        "        logging.info(f\"‚úÖ Loaded {len(api_keys)} API keys.\")\n",
        "    except Exception as e: raise ValueError(f\"Could not load/validate '{keys_str}': {e}\")\n",
        "    prompt_text = (workspace_path / prompt_str).read_text(encoding='utf-8').strip()\n",
        "    if not prompt_text: raise ValueError(f\"'{prompt_str}' is empty.\")\n",
        "    if not shutil.which(\"ffprobe\"): raise EnvironmentError(\"ffprobe not found.\")\n",
        "    config = Config(workspace_path=workspace_path, keys_filename=keys_str, prompt_filename=prompt_str, output_suffix=OUTPUT_FILENAME_SUFFIX, model_name=MODEL_NAME, prompt_template=prompt_text, chunk_duration_seconds=chunk_min * 60)\n",
        "    config.results_path.mkdir(exist_ok=True); config.temp_chunk_path.mkdir(exist_ok=True)\n",
        "    return ValidatedSetup(config=config, api_keys=api_keys, log_file_path=log_file)\n",
        "\n",
        "def get_media_duration(file_path: Path) -> float:\n",
        "    command = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', str(file_path)]\n",
        "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "    return float(result.stdout.strip())\n",
        "\n",
        "# --- REWRITTEN run_pipeline FUNCTION TO HANDLE SANITIZED FILENAMES ---\n",
        "def run_pipeline(setup: ValidatedSetup) -> Dict[str, Path]:\n",
        "    config, api_keys, log_file_path = setup\n",
        "    VALID_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac']\n",
        "    file_info_cache = {}\n",
        "    safe_stem_map = {} # Maps safe_stem back to original Path object\n",
        "\n",
        "    all_input_paths = [f for f in config.input_path.iterdir() if f.is_file() and f.suffix.lower() in VALID_EXTENSIONS]\n",
        "    for f in all_input_paths:\n",
        "        safe_stem = sanitize_stem(f.stem)\n",
        "        safe_stem_map[safe_stem] = f\n",
        "\n",
        "    if FORCE_REPROCESS:\n",
        "        logging.warning(\"üî• FORCE REPROCESS ENABLED: All existing final results will be ignored.\")\n",
        "        existing_final_results = set()\n",
        "    else:\n",
        "        existing_final_results = {\n",
        "            sanitize_stem(f.stem) for f in all_input_paths\n",
        "            if (config.results_path / f\"{sanitize_stem(f.stem)}{config.output_suffix}.txt\").exists()\n",
        "        }\n",
        "\n",
        "    all_input_files_to_process = [f for f in all_input_paths if sanitize_stem(f.stem) not in existing_final_results]\n",
        "\n",
        "    if not all_input_files_to_process:\n",
        "        logging.info(\"‚úÖ No new input files to process.\")\n",
        "        return safe_stem_map\n",
        "\n",
        "    all_tasks = []\n",
        "    logging.info(f\"Found {len(all_input_files_to_process)} new files to process. Analyzing and preparing tasks...\")\n",
        "    for file in all_input_files_to_process:\n",
        "        try:\n",
        "            safe_stem = sanitize_stem(file.stem)\n",
        "            duration = get_media_duration(file)\n",
        "            num_chunks = math.ceil(duration / config.chunk_duration_seconds)\n",
        "            file_info_cache[safe_stem] = FileInfo(duration=duration, expected_chunks=num_chunks)\n",
        "            existing_temp_chunks = {f.name for f in config.temp_chunk_path.glob(f\"{safe_stem}_chunk_*.txt\")}\n",
        "            file_tasks_count = 0\n",
        "            for i in range(num_chunks):\n",
        "                chunk_filename = f\"{safe_stem}_chunk_{i}{config.output_suffix}.txt\"\n",
        "                if chunk_filename not in existing_temp_chunks:\n",
        "                    offset = i * config.chunk_duration_seconds\n",
        "                    all_tasks.append(ProcessTask(media_path=file, original_stem=safe_stem, chunk_index=i, time_offset_seconds=offset))\n",
        "                    file_tasks_count += 1\n",
        "            logging.info(f\" -> Found {file_tasks_count} new chunks to process for '{file.name}'.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to prepare tasks for {file.name}: {e}\")\n",
        "\n",
        "    if not all_tasks:\n",
        "        logging.info(\"‚úÖ All chunks for all files are already processed.\")\n",
        "    else:\n",
        "        num_workers = min(NUM_WORKERS, len(api_keys), len(all_tasks))\n",
        "        logging.info(f\"\\nStarting processing for {len(all_tasks)} total new chunks with {num_workers} parallel workers.\")\n",
        "        tasks_to_process = all_tasks\n",
        "        for attempt in range(MAX_RETRIES + 1):\n",
        "            if not tasks_to_process: break\n",
        "            if attempt > 0:\n",
        "                logging.warning(f\"\\n--- üîÅ Retrying {len(tasks_to_process)} failed chunks (Attempt {attempt + 1}/{MAX_RETRIES + 1}) ---\")\n",
        "                time.sleep(5)\n",
        "            failed_tasks_this_round = []\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "                futures = {executor.submit(process_chunk_worker, task, config, api_keys, file_info_cache): task for task in tasks_to_process}\n",
        "                progress_desc = f\"Processing Chunks (Attempt {attempt+1})\"\n",
        "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(tasks_to_process), desc=progress_desc):\n",
        "                    try:\n",
        "                        status, task = future.result()\n",
        "                        if status == 'failure': failed_tasks_this_round.append(task)\n",
        "                    except Exception as exc:\n",
        "                        task = futures[future]\n",
        "                        logging.error(f\"A worker thread for task [{task.original_stem}_chunk_{task.chunk_index}] raised an unhandled exception: {exc}\")\n",
        "                        failed_tasks_this_round.append(task)\n",
        "            tasks_to_process = failed_tasks_this_round\n",
        "\n",
        "    logging.info(\"\\n--- Final check for any completed files to patch ---\")\n",
        "    stems_with_chunks = {re.match(r'(.+)_chunk_', f.name).group(1) for f in config.temp_chunk_path.iterdir() if re.match(r'(.+)_chunk_', f.name)}\n",
        "    final_check_cache = file_info_cache.copy()\n",
        "    for stem in stems_with_chunks:\n",
        "        if stem not in final_check_cache:\n",
        "            try:\n",
        "                original_file = safe_stem_map.get(stem)\n",
        "                if not original_file: raise StopIteration\n",
        "                duration = get_media_duration(original_file)\n",
        "                num_chunks = math.ceil(duration / config.chunk_duration_seconds)\n",
        "                final_check_cache[stem] = FileInfo(duration=duration, expected_chunks=num_chunks)\n",
        "            except (StopIteration, Exception) as e:\n",
        "                logging.warning(f\"Could not find original input for safe_stem '{stem}' during final check: {e}\")\n",
        "    for stem in final_check_cache:\n",
        "        try_patch_file(stem, config, final_check_cache)\n",
        "    logging.info(\"Final check complete.\")\n",
        "    return safe_stem_map\n",
        "\n",
        "# --- 4. SCRIPT ENTRY POINT ---\n",
        "if __name__ == '__main__':\n",
        "    if 'google.colab' in sys.modules and ENABLE_KEEP_ALIVE:\n",
        "        js_code = \"function ClickConnect(){ console.log('Keeping Colab session active...'); document.querySelector('colab-connect-button').click(); } setInterval(ClickConnect, 60000);\"\n",
        "        display(HTML(f\"<script>{js_code}</script>\")); print(\"‚úÖ Runtime Keep-Alive enabled.\")\n",
        "\n",
        "    log_file_path = None\n",
        "    all_targeted_files_map = {}\n",
        "    try:\n",
        "        validated_setup = setup_and_validate(WORKSPACE_PATH, KEYS_FILENAME, PROMPT_FILENAME, CHUNK_DURATION_MINUTES)\n",
        "        log_file_path = validated_setup.log_file_path\n",
        "        all_targeted_files_map = run_pipeline(validated_setup)\n",
        "        logging.info(\"‚úÖ Script finished successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüö® SCRIPT HALTED: {e}\")\n",
        "        if logging.getLogger().hasHandlers():\n",
        "            logging.critical(f\"SCRIPT HALTED: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        # --- REWRITTEN SUMMARY LOGIC ---\n",
        "        print(\"\\n--- üìä Final Processing Summary ---\")\n",
        "        if not all_targeted_files_map:\n",
        "             print(\"No input files were found or targeted in this run.\")\n",
        "        else:\n",
        "             config_for_summary = Config(workspace_path=Path(WORKSPACE_PATH), keys_filename=\"\", prompt_filename=\"\", output_suffix=OUTPUT_FILENAME_SUFFIX, model_name=\"\", prompt_template=\"\", chunk_duration_seconds=0)\n",
        "             processed_safe_stems = {f.stem.replace(config_for_summary.output_suffix, \"\") for f in config_for_summary.results_path.glob(f\"*{config_for_summary.output_suffix}.txt\")}\n",
        "\n",
        "             succeeded_files = []\n",
        "             failed_files = []\n",
        "\n",
        "             for safe_stem, original_path in all_targeted_files_map.items():\n",
        "                 if safe_stem in processed_safe_stems:\n",
        "                     succeeded_files.append(original_path.name)\n",
        "                 else:\n",
        "                     failed_files.append(original_path.name)\n",
        "\n",
        "             if succeeded_files:\n",
        "                print(\"\\n‚úÖ Successfully Processed and Patched:\")\n",
        "                for f_name in sorted(succeeded_files):\n",
        "                    print(f\"  - {f_name}\")\n",
        "             if failed_files:\n",
        "                print(\"\\nüö® Files That May Have Failed (result not found):\")\n",
        "                for f_name in sorted(failed_files):\n",
        "                    print(f\"  - {f_name}\")\n",
        "\n",
        "        print(\"\\n------------------------------------\")\n",
        "        if log_file_path and log_file_path.exists():\n",
        "            print(f\"\\n--> Run ended. Detailed log available at: {log_file_path}\")\n",
        "        else:\n",
        "            print(\"\\n--> Run ended.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AiJv_JMwd9wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ===================================================================================\n",
        "#\n",
        "#  SRT SPLITTER & CLEANER (FINAL - ROBUST TIMESTAMP PARSING)\n",
        "#\n",
        "# ===================================================================================\n",
        "# @markdown # **Step 1: Define Your Workspace**\n",
        "# @markdown Confirm your main folder path and source file suffix below.\n",
        "# @markdown ---\n",
        "WORKSPACE_PATH = \"/content/drive/MyDrive/GeminiProcessor\" # @param {type:\"string\"}\n",
        "SOURCE_FILE_SUFFIX = \"_result\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown # **Step 4: Advanced Line Splitting (Readability-Aware)**\n",
        "# @markdown ---\n",
        "# @markdown Enable this to automatically split subtitles that are too long or too fast to read.\n",
        "SPLIT_LONG_LINES = True #@param {type:\"boolean\"}\n",
        "# @markdown Lines longer than this character count will be split.\n",
        "MAX_LINE_LENGTH = 48 #@param {type:\"integer\"}\n",
        "# @markdown Subtitles requiring a reading speed faster than this will be split.\n",
        "# @markdown (CPS = Characters Per Second. Recommended: 15-22)\n",
        "MAX_READING_SPEED_CPS = 20 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "# ===================================================================================\n",
        "#  --- Initial Setup: Mount Google Drive ---\n",
        "# ===================================================================================\n",
        "import os, math\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "DRIVE_MOUNTED = False\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    workspace_check = Path(WORKSPACE_PATH)\n",
        "    if not workspace_check.is_dir():\n",
        "        print(f\"‚ö†Ô∏è Warning: WORKSPACE_PATH does not exist: {WORKSPACE_PATH}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Google Drive mounted. Workspace ready at: {WORKSPACE_PATH}\")\n",
        "        DRIVE_MOUNTED = True\n",
        "except Exception as e:\n",
        "    print(f\"üö® Failed to mount Google Drive: {e}\")\n",
        "\n",
        "# ===================================================================================\n",
        "#  --- Main Application Engine & UI ---\n",
        "# ===================================================================================\n",
        "if DRIVE_MOUNTED:\n",
        "    import re, logging, textwrap, sys\n",
        "    from tqdm.notebook import tqdm\n",
        "    import ipywidgets as widgets\n",
        "\n",
        "    # --- SETUP & GLOBAL CONSTANTS ---\n",
        "    CHAR_SETS_MAP = {\n",
        "        \"Russian/Cyrillic\": set(\"–ê–∞–ë–±–í–≤–ì–≥–î–¥–ï–µ–Å—ë–ñ–∂–ó–∑–ò–∏–ô–π–ö–∫–õ–ª–ú–º–ù–Ω–û–æ–ü–ø–†—Ä–°—Å–¢—Ç–£—É–§—Ñ–•—Ö–¶—Ü–ß—á–®—à–©—â–™—ä–´—ã–¨—å–≠—ç–Æ—é–Ø—è\"),\n",
        "        \"Chinese/Japanese/Korean\": set(\"\".join(chr(c) for c in range(0x4E00, 0x9FFF + 1))),\n",
        "        \"Hindi/Devanagari\": set(\"\".join(chr(c) for c in range(0x0900, 0x097F + 1))),\n",
        "        \"Arabic\": set(\"\".join(chr(c) for c in range(0x0600, 0x06FF + 1))),\n",
        "        \"Emojis/Symbols\": set(\"\".join(chr(c) for c in range(0x1F300, 0x1F64F + 1)) + \"‚òÖ‚òÜ‚ô™‚ô´\")\n",
        "    }\n",
        "\n",
        "    # --- CORE PROCESSING LOGIC ---\n",
        "    def setup_logging(workspace_path):\n",
        "        log_path = Path(workspace_path) / \"srt_splitter_log.txt\"\n",
        "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', filename=log_path, filemode='w')\n",
        "        if not any(isinstance(h, logging.StreamHandler) for h in logging.getLogger('').handlers):\n",
        "            console = logging.StreamHandler(sys.stdout); console.setLevel(logging.INFO)\n",
        "            console.setFormatter(logging.Formatter('%(message)s')); logging.getLogger('').addHandler(console)\n",
        "\n",
        "    # --- CORRECTED Timecode Helper Functions ---\n",
        "    def srt_time_to_ms(time_str):\n",
        "        \"\"\"Converts SRT time string to milliseconds, handling both HH:MM:SS,ms and MM:SS,ms.\"\"\"\n",
        "        # If no hours component is present, add it.\n",
        "        if time_str.count(':') == 1:\n",
        "            time_str = \"00:\" + time_str\n",
        "\n",
        "        h, m, s, ms = map(int, re.split('[:,]', time_str))\n",
        "        return (h * 3600 + m * 60 + s) * 1000 + ms\n",
        "\n",
        "    def ms_to_srt_time(ms_total):\n",
        "        ms = ms_total % 1000; total_seconds = ms_total // 1000\n",
        "        s = total_seconds % 60; m = (total_seconds // 60) % 60\n",
        "        h = total_seconds // 3600\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
        "\n",
        "    def should_split_subtitle(text, start_ms, end_ms, max_len, max_cps):\n",
        "        duration_sec = (end_ms - start_ms) / 1000.0\n",
        "        if duration_sec <= 0: return False\n",
        "        cps = len(text) / duration_sec\n",
        "        return len(text) > max_len or cps > max_cps\n",
        "\n",
        "    def split_subtitle_by_readability(text, start_ms, end_ms, max_len):\n",
        "        lines = textwrap.wrap(text, width=max_len, break_long_words=False, replace_whitespace=False)\n",
        "        if len(lines) <= 1: return [(text, ms_to_srt_time(start_ms), ms_to_srt_time(end_ms))]\n",
        "\n",
        "        total_duration_ms = end_ms - start_ms\n",
        "        total_chars = len(\"\".join(lines))\n",
        "\n",
        "        new_blocks = []; current_time_ms = start_ms\n",
        "        for i, line in enumerate(lines):\n",
        "            char_proportion = len(line) / total_chars if total_chars > 0 else 1/len(lines)\n",
        "            chunk_duration_ms = total_duration_ms * char_proportion\n",
        "\n",
        "            chunk_start_ms = current_time_ms\n",
        "            chunk_end_ms = end_ms if i == len(lines) - 1 else current_time_ms + chunk_duration_ms\n",
        "\n",
        "            new_blocks.append((line, ms_to_srt_time(int(chunk_start_ms)), ms_to_srt_time(int(chunk_end_ms))))\n",
        "            current_time_ms = chunk_end_ms\n",
        "        return new_blocks\n",
        "\n",
        "    def process_single_file(filepath, config):\n",
        "        try:\n",
        "            content = filepath.read_text(encoding='utf-8').strip()\n",
        "            if not content: return \"skipped_empty\"\n",
        "            content = re.sub(r':(\\d{3})\\s*-->', r',\\1 -->', content)\n",
        "            content = re.sub(r':(\\d{3})$', r',\\1', content, flags=re.MULTILINE)\n",
        "            matches = list(re.finditer(r'(?:\\d{2}:)?\\d{2}:\\d{2},\\d{3}\\s*-->\\s*(?:\\d{2}:)?\\d{2}:\\d{2},\\d{3}', content))\n",
        "            if not matches: raise ValueError(\"No valid timestamps found.\")\n",
        "\n",
        "            srt_data = {p: [] for p in config['languages']}; blocklist = config.get('blocklist', set())\n",
        "            for i, match in enumerate(matches):\n",
        "                timestamp = match.group(0); start, end = match.end(), matches[i+1].start() if i+1 < len(matches) else len(content)\n",
        "                texts = {p: \"\" for p in config['languages']}\n",
        "                for line in content[start:end].strip().split('\\n'):\n",
        "                    if line.strip():\n",
        "                        for p in config['languages']:\n",
        "                            if line.startswith(p) and ':' in line: texts[p] = line.split(':', 1)[1].strip(); break\n",
        "                if not any(texts.values()): continue\n",
        "\n",
        "                for p, t in texts.items():\n",
        "                    clean_t = \"\".join(c for c in t if c not in blocklist) if blocklist else t\n",
        "                    if not clean_t: continue\n",
        "\n",
        "                    if config['split_long_lines']:\n",
        "                        start_str, end_str = timestamp.split(' --> ')\n",
        "                        start_ms, end_ms = srt_time_to_ms(start_str), srt_time_to_ms(end_str)\n",
        "\n",
        "                        if should_split_subtitle(clean_t, start_ms, end_ms, config['max_line_length'], config['max_cps']):\n",
        "                            new_blocks = split_subtitle_by_readability(clean_t, start_ms, end_ms, config['max_line_length'])\n",
        "                            for text_chunk, start_chunk_str, end_chunk_str in new_blocks:\n",
        "                                new_ts = f\"{start_chunk_str} --> {end_chunk_str}\"\n",
        "                                srt_data[p].append(f\"{len(srt_data[p]) + 1}\\n{new_ts}\\n{text_chunk}\")\n",
        "                        else:\n",
        "                            srt_data[p].append(f\"{len(srt_data[p]) + 1}\\n{timestamp}\\n{clean_t}\")\n",
        "                    else:\n",
        "                        srt_data[p].append(f\"{len(srt_data[p]) + 1}\\n{timestamp}\\n{clean_t}\")\n",
        "\n",
        "            if not any(srt_data.values()): raise ValueError(\"No valid blocks were extracted.\")\n",
        "            base_name = filepath.stem.replace(config['source_suffix'], '')\n",
        "            for p, blocks in srt_data.items():\n",
        "                if blocks: (config['output_dir'] / f\"{base_name}{config['languages'][p]}.srt\").write_text('\\n\\n'.join(blocks) + '\\n', 'utf-8')\n",
        "            return \"success\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process '{filepath.name}': {e}\"); return \"failed\"\n",
        "\n",
        "    # --- DYNAMIC UI WIDGETS AND FUNCTIONS ---\n",
        "    lang_checkboxes, char_checkboxes = {}, {}\n",
        "    analyze_button = widgets.Button(description=\"Analyze Files\", button_style='primary', icon='search', layout=widgets.Layout(width='200px'))\n",
        "    process_button = widgets.Button(description=\"Run Processing\", button_style='success', icon='cogs', layout=widgets.Layout(width='200px', visibility='hidden'))\n",
        "    lang_box, char_box, status_area = widgets.VBox(), widgets.VBox(), widgets.Output()\n",
        "\n",
        "    def on_analyze_click(b):\n",
        "        b.disabled = True; lang_box.children, char_box.children = [], []; process_button.layout.visibility = 'hidden'\n",
        "        with status_area:\n",
        "            clear_output(wait=True); print(\"üöÄ Analyzing files...\")\n",
        "            try:\n",
        "                workspace = Path(WORKSPACE_PATH); results_dir = workspace / \"results\"\n",
        "                dirs_to_scan = [results_dir, results_dir / \"chunk_results\"]\n",
        "                files = [p for s_dir in dirs_to_scan if s_dir.is_dir() for p in s_dir.glob(f\"*{SOURCE_FILE_SUFFIX}.txt\")]\n",
        "                if not files: print(f\"‚ùå No source files found ending with '{SOURCE_FILE_SUFFIX}.txt'.\"); b.disabled = False; return\n",
        "\n",
        "                prefixes, chars = set(), set()\n",
        "                for f in files[:10]:\n",
        "                    content = f.read_text('utf-8')\n",
        "                    prefixes.update(re.findall(r'^\\s*([a-zA-Z]{1,4}:)', content, re.MULTILINE))\n",
        "                    for char in content:\n",
        "                        for name, char_set in CHAR_SETS_MAP.items():\n",
        "                            if char in char_set: chars.add(name)\n",
        "                if not prefixes: print(\"‚ö†Ô∏è Could not detect language prefixes (like 'en:').\"); b.disabled = False; return\n",
        "\n",
        "                global lang_checkboxes, char_checkboxes\n",
        "                lang_checkboxes = {p: widgets.Checkbox(v=True, description=p, indent=False) for p in sorted(list(prefixes))}\n",
        "                char_checkboxes = {c: widgets.Checkbox(v=False, description=c, indent=False) for c in sorted(list(chars))}\n",
        "                lang_box.children = [widgets.HTML(\"<b>Step 2: Select languages to extract</b>\")] + list(lang_checkboxes.values())\n",
        "                if char_checkboxes: char_box.children = [widgets.HTML(\"<b>Step 3: Select character sets to remove</b>\")] + list(char_checkboxes.values())\n",
        "                process_button.layout.visibility = 'visible'\n",
        "                clear_output(wait=True); print(\"‚úÖ Analysis complete. Review options and click 'Run Processing'.\")\n",
        "            except Exception as e: clear_output(wait=True); print(f\"üö® Analysis Error: {e}\")\n",
        "        b.disabled = False\n",
        "\n",
        "    def on_process_click(b):\n",
        "        b.disabled = True; analyze_button.disabled = True\n",
        "        for cb in lang_checkboxes.values(): cb.disabled = True;\n",
        "        for cb in char_checkboxes.values(): cb.disabled = True\n",
        "        with status_area:\n",
        "            clear_output(wait=True)\n",
        "            try:\n",
        "                print(\"‚öôÔ∏è Preparing to process...\"); selected_prefixes = [p for p, cb in lang_checkboxes.items() if cb.value]\n",
        "                if not selected_prefixes: print(\"‚ùå No languages selected.\"); b.disabled = False; analyze_button.disabled = False; return\n",
        "\n",
        "                languages = {p: f\"_{p.replace(':', '').lower()}\" for p in selected_prefixes}\n",
        "                selected_chars = [c for c, cb in char_checkboxes.items() if cb.value]\n",
        "                blocklist = set().union(*(CHAR_SETS_MAP[name] for name in selected_chars))\n",
        "                workspace = Path(WORKSPACE_PATH); setup_logging(workspace)\n",
        "                logging.info(f\"Languages: {languages}\"); logging.info(f\"Chars to remove: {selected_chars or 'None'}\")\n",
        "\n",
        "                config = { \"source_suffix\": SOURCE_FILE_SUFFIX, \"languages\": languages, \"blocklist\": blocklist,\n",
        "                           \"split_long_lines\": SPLIT_LONG_LINES, \"max_line_length\": MAX_LINE_LENGTH, \"max_cps\": MAX_READING_SPEED_CPS,\n",
        "                           \"output_dir\": (workspace / \"results\" / \"final_srt_files\")}\n",
        "                config['output_dir'].mkdir(exist_ok=True)\n",
        "\n",
        "                results_dir = workspace / \"results\"; dirs_to_scan = [results_dir, results_dir / \"chunk_results\"]\n",
        "                files = [p for s_dir in dirs_to_scan if s_dir.is_dir() for p in s_dir.glob(f\"*{config['source_suffix']}.txt\")]\n",
        "\n",
        "                print(f\"Found {len(files)} files. Starting...\"); success_count = 0\n",
        "                for f in tqdm(files, desc=\"Processing Files\"):\n",
        "                    if process_single_file(f, config) == \"success\": success_count += 1\n",
        "\n",
        "                print(\"\\n\" + \"=\"*50); print(\" PROCESSING COMPLETE\".center(50)); print(\"=\"*50)\n",
        "                print(f\"  ‚úÖ Successfully processed: {success_count} files\")\n",
        "                print(f\"  üö® Failed or skipped: {len(files) - success_count} files\")\n",
        "                print(f\"  üìÇ Final SRTs saved in: {config['output_dir']}\")\n",
        "                print(f\"  ‚ÑπÔ∏è Log saved at: {workspace / 'srt_splitter_log.txt'}\")\n",
        "            except Exception as e: clear_output(wait=True); print(f\"üö® Processing Error: {e}\")\n",
        "\n",
        "    # --- Attach Functions to Buttons & Display UI ---\n",
        "    analyze_button.on_click(on_analyze_click)\n",
        "    process_button.on_click(on_process_click)\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<b>Click 'Analyze Files' to configure the processor.</b>\"),\n",
        "        widgets.HBox([analyze_button, process_button]), lang_box, char_box, status_area\n",
        "    ]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1Ozb_vhzfMa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "axToem9dJQ78"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @markdown ### ‚öôÔ∏è Step 5: Definitive SRT Splitter (Handles All Known Errors)\n",
        "# @markdown This final version is the most robust. It automatically:\n",
        "# @markdown 1.  Fixes invalid timestamp formats (e.g., `SS:ms` -> `SS,ms`).\n",
        "# @markdown 2.  Handles both `HH:MM:SS,ms` and `MM:SS,ms` timestamps.\n",
        "# @markdown 3.  Repairs missing SRT index numbers.\n",
        "# @markdown 4.  Ignores incomplete \"garbage\" blocks at the end of a file.\n",
        "# @markdown 5.  Saves all final, clean SRTs to a dedicated output folder.\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# --- Configuration (Self-Contained) ---\n",
        "# @markdown **Ensure these values match the settings in your main script!**\n",
        "WORKSPACE_PATH = \"/content/drive/MyDrive/GeminiProcessor\" # @param {type:\"string\"}\n",
        "OUTPUT_FILENAME_SUFFIX = \"_result\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown **Define the suffixes and the dedicated output folder.**\n",
        "FINAL_SRT_FOLDER_NAME = \"final_srt_files\" # @param {type:\"string\"}\n",
        "PHONETIC_FILE_SUFFIX = \"_phonetic\" # @param {type:\"string\"}\n",
        "FARSI_FILE_SUFFIX = \"_farsi\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Check this box to delete the original `.txt` files after a successful split.\n",
        "DELETE_ORIGINAL_TXT_FILES = False # @param {type:\"boolean\"}\n",
        "\n",
        "def srt_splitter_main():\n",
        "    \"\"\"\n",
        "    Finds dual-language text files, normalizes common AI errors, splits them\n",
        "    into separate SRT files, saves to a dedicated folder, and cleans up.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Definitive Dual-Language SRT Splitter ---\")\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "        workspace = Path(WORKSPACE_PATH)\n",
        "        results_dir = workspace / \"results\"\n",
        "        chunk_results_dir = results_dir / \"chunk_results\"\n",
        "\n",
        "        final_output_dir = results_dir / FINAL_SRT_FOLDER_NAME\n",
        "        final_output_dir.mkdir(exist_ok=True)\n",
        "        print(f\"‚úÖ Final SRT files will be saved to: '{final_output_dir.relative_to(workspace)}'\")\n",
        "\n",
        "        dirs_to_scan = [chunk_results_dir, results_dir]\n",
        "        total_processed = 0\n",
        "        total_skipped = 0\n",
        "\n",
        "        for source_dir in dirs_to_scan:\n",
        "            if not source_dir.is_dir() or source_dir == final_output_dir:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nüîç Scanning directory: '{source_dir.relative_to(workspace)}'...\")\n",
        "            glob_pattern = f\"*{OUTPUT_FILENAME_SUFFIX}.txt\"\n",
        "            txt_files = list(source_dir.glob(glob_pattern))\n",
        "\n",
        "            if not txt_files:\n",
        "                print(f\"  ‚úÖ No '{glob_pattern}' files found to process here.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  Found {len(txt_files)} source files to process.\")\n",
        "            for txt_path in txt_files:\n",
        "                try:\n",
        "                    content = txt_path.read_text(encoding='utf-8').strip()\n",
        "                    if not content:\n",
        "                        print(f\"  ‚ö†Ô∏è Skipping '{txt_path.name}': File is empty.\")\n",
        "                        total_skipped += 1\n",
        "                        continue\n",
        "\n",
        "                    # --- NORMALIZATION STEP 1: Fix invalid timestamps (SS:ms -> SS,ms) ---\n",
        "                    # Use a regex to find a colon followed by 3 digits right before the '-->'\n",
        "                    content = re.sub(r':(\\d{3})\\s*-->', r',\\1 -->', content)\n",
        "                    # And do the same for the end timestamp\n",
        "                    content = re.sub(r':(\\d{3})$', r',\\1', content, flags=re.MULTILINE)\n",
        "\n",
        "                    phonetic_srt_blocks = []\n",
        "                    farsi_srt_blocks = []\n",
        "\n",
        "                    timestamp_pattern = r'(?:\\d{2}:)?\\d{2}:\\d{2},\\d{3}\\s*-->\\s*(?:\\d{2}:)?\\d{2}:\\d{2},\\d{3}'\n",
        "                    matches = list(re.finditer(timestamp_pattern, content))\n",
        "\n",
        "                    if not matches:\n",
        "                        raise ValueError(\"After normalization, still no valid timestamps found.\")\n",
        "\n",
        "                    current_index = 1\n",
        "                    for i, match in enumerate(matches):\n",
        "                        timestamp = match.group(0)\n",
        "                        start_pos = match.end()\n",
        "                        end_pos = matches[i+1].start() if i + 1 < len(matches) else len(content)\n",
        "                        block_text = content[start_pos:end_pos].strip()\n",
        "                        text_lines = [line for line in block_text.split('\\n') if line.strip()]\n",
        "\n",
        "                        # --- NORMALIZATION STEP 2: Handle incomplete final blocks ---\n",
        "                        if len(text_lines) < 2:\n",
        "                            print(f\"    ‚ö†Ô∏è Warning: Incomplete block after '{timestamp}' in '{txt_path.name}'. Ignoring it.\")\n",
        "                            continue # Skip this malformed block and continue\n",
        "\n",
        "                        line_i = text_lines[0]\n",
        "                        line_f = text_lines[1]\n",
        "\n",
        "                        if not line_i.strip().startswith('I:') or not line_f.strip().startswith('f:'):\n",
        "                            print(f\"    ‚ö†Ô∏è Warning: Block after '{timestamp}' in '{txt_path.name}' lacks prefixes. Ignoring it.\")\n",
        "                            continue # Skip this malformed block\n",
        "\n",
        "                        text_i = line_i.split(':', 1)[1].strip()\n",
        "                        text_f = line_f.split(':', 1)[1].strip()\n",
        "\n",
        "                        phonetic_srt_blocks.append(f\"{current_index}\\n{timestamp}\\n{text_i}\")\n",
        "                        farsi_srt_blocks.append(f\"{current_index}\\n{timestamp}\\n{text_f}\")\n",
        "                        current_index += 1\n",
        "\n",
        "                    # Ensure we actually processed something before saving\n",
        "                    if not phonetic_srt_blocks:\n",
        "                         raise ValueError(\"No valid, complete blocks could be extracted from the file.\")\n",
        "\n",
        "                    base_name = txt_path.stem.replace(OUTPUT_FILENAME_SUFFIX, '')\n",
        "\n",
        "                    phonetic_srt_path = final_output_dir / f\"{base_name}{PHONETIC_FILE_SUFFIX}.srt\"\n",
        "                    phonetic_srt_path.write_text('\\n\\n'.join(phonetic_srt_blocks), encoding='utf-8')\n",
        "\n",
        "                    farsi_srt_path = final_output_dir / f\"{base_name}{FARSI_FILE_SUFFIX}.srt\"\n",
        "                    farsi_srt_path.write_text('\\n\\n'.join(farsi_srt_blocks), encoding='utf-8')\n",
        "\n",
        "                    print(f\"  ‚úÖ Processed '{txt_path.name}' -> Saved 2 SRTs in '{FINAL_SRT_FOLDER_NAME}'\")\n",
        "                    total_processed += 1\n",
        "\n",
        "                    if DELETE_ORIGINAL_TXT_FILES:\n",
        "                        txt_path.unlink()\n",
        "                        print(f\"    üóëÔ∏è  Deleted original source file.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  üö® Error processing '{txt_path.name}': {e}. Skipping this file.\")\n",
        "                    total_skipped += 1\n",
        "                    continue\n",
        "\n",
        "        print(\"\\n--- Splitter Process Summary ---\")\n",
        "        print(f\"  Successfully processed and split: {total_processed} files\")\n",
        "        print(f\"  Skipped or failed: {total_skipped} files\")\n",
        "        if DELETE_ORIGINAL_TXT_FILES and total_processed > 0:\n",
        "            print(\"  Cleanup of original .txt source files is complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüö® A FATAL SCRIPT ERROR OCCURRED: {e}\")\n",
        "    finally:\n",
        "        print(\"\\n--> SRT splitter script finished.\")\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "if __name__ == '__main__':\n",
        "    srt_splitter_main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title ü§ñ Power-User Downloader (Genuinely Smart URL Detection)\n",
        "# @markdown ### New Feature: \"Telegram Message Detective\"\n",
        "# @markdown The URL detection is now far more powerful. It can find URLs hidden in:\n",
        "# @markdown - **Hyperlinks** (blue, clickable text)\n",
        "# @markdown - **Captions** of photos and videos\n",
        "# @markdown - **Link Previews** and forwarded messages\n",
        "\n",
        "# =====================================================================================\n",
        "# @markdown # ü§ñ BOT CREDENTIALS\n",
        "# =====================================================================================\n",
        "TELEGRAM_BOT_TOKEN = \"8386939225:AAE8HKCcY-fwgxc4b_eRdZ5J1J6ToBhjWsk\"  # @param {type:\"string\"}\n",
        "TELEGRAM_CHAT_ID = \"5123471319\"  # @param {type:\"string\"}\n",
        "\n",
        "# =====================================================================================\n",
        "# @markdown # ‚öôÔ∏è DOWNLOAD SETTINGS & COMMANDS\n",
        "# =====================================================================================\n",
        "OUTPUT_DIRECTORY = \"/content/drive/MyDrive/YT-DLP Downloads\"  # @param {type:\"string\"}\n",
        "FILENAME_TEMPLATE = \"%(uploader)s - %(title)s [%(resolution)s] [%(id)s].%(ext)s\" # @param {type:\"string\"}\n",
        "VIDEO_QUALITY = \"Best Available\" # @param [\"Best Available\", \"1080p (Full HD)\", \"720p (HD)\"]\n",
        "POST_PROCESSING = \"None\" #@param [\"None\", \"Extract Audio (mp3)\"]\n",
        "ADDITIONAL_YTDLP_FLAGS = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# =====================================================================================\n",
        "# SCRIPT (Do not modify below this line)\n",
        "# =====================================================================================\n",
        "import os\n",
        "import shlex\n",
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Install Dependencies ---\n",
        "print(\"Step 1: Installing dependencies...\")\n",
        "os.system('pip install -q --force-reinstall --no-deps \"https://github.com/yt-dlp/yt-dlp/archive/master.zip\"')\n",
        "os.system('pip install -q urlextract')\n",
        "print(\"‚úÖ Dependencies installed.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "from urlextract import URLExtract\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def send_telegram_message(text):\n",
        "    if TELEGRAM_BOT_TOKEN == \"PASTE_YOUR_BOT_TOKEN_HERE\": return None\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"; payload = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": text, \"parse_mode\": \"Markdown\", \"disable_web_page_preview\": True}\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=10)\n",
        "        return response.json()['result']['message_id'] if response.ok else None\n",
        "    except: return None\n",
        "\n",
        "def edit_telegram_message(text, message_id):\n",
        "    if not message_id: return\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/editMessageText\"; payload = {\"chat_id\": TELEGRAM_CHAT_ID, \"message_id\": message_id, \"text\": text, \"parse_mode\": \"Markdown\", \"disable_web_page_preview\": True}\n",
        "    try: requests.post(url, json=payload, timeout=10)\n",
        "    except: pass\n",
        "\n",
        "# <-- THE NEW, SMARTER URL DETECTIVE FUNCTION ---\n",
        "def get_urls_from_update(update):\n",
        "    \"\"\"\n",
        "    Intelligently extracts URLs from all possible fields in a Telegram update object,\n",
        "    including text, captions, and hyperlink entities.\n",
        "    \"\"\"\n",
        "    if not update: return []\n",
        "\n",
        "    urls = set() # Use a set to automatically handle duplicates\n",
        "    extractor = URLExtract()\n",
        "\n",
        "    # Check the primary message object, or an edited message object\n",
        "    message = update.get('edited_message') or update.get('message')\n",
        "    if not message: return []\n",
        "\n",
        "    # 1. Extract from plain text or caption\n",
        "    text_content = message.get('text') or message.get('caption')\n",
        "    if text_content:\n",
        "        found_in_text = extractor.find_urls(text_content)\n",
        "        urls.update(found_in_text)\n",
        "\n",
        "    # 2. Extract from message entities (for hyperlinks)\n",
        "    entities = message.get('entities') or message.get('caption_entities')\n",
        "    if entities and text_content:\n",
        "        for entity in entities:\n",
        "            if entity['type'] == 'text_link':\n",
        "                urls.add(entity['url'])\n",
        "            elif entity['type'] == 'url':\n",
        "                # For plain URLs that Telegram also marks as entities\n",
        "                offset = entity['offset']\n",
        "                length = entity['length']\n",
        "                urls.add(text_content[offset:offset+length])\n",
        "\n",
        "    return list(urls)\n",
        "\n",
        "def get_domain_keyword(url):\n",
        "    try:\n",
        "        netloc = urlparse(url).netloc.lower()\n",
        "        known_keywords = ['youtube', 'youtu.be', 'instagram', 'twitter', 'facebook', 'vimeo', 'dailymotion', 'twitch', 'x.com']\n",
        "        for keyword in known_keywords:\n",
        "            if keyword in netloc:\n",
        "                if keyword == 'youtu.be': return 'youtube'\n",
        "                if keyword == 'x.com': return 'twitter'\n",
        "                return keyword\n",
        "        return netloc.replace('www.', '').split('.')[0]\n",
        "    except: return None\n",
        "\n",
        "def find_cookie_file_by_keyword(keyword):\n",
        "    if not keyword: return None, None\n",
        "    drive_root = '/content/drive/MyDrive/'\n",
        "    try:\n",
        "        for filename in os.listdir(drive_root):\n",
        "            full_path = os.path.join(drive_root, filename)\n",
        "            if not os.path.isfile(full_path): continue\n",
        "            if not (filename.lower().endswith('.txt') or filename.lower().endswith('.json')): continue\n",
        "            if keyword.lower() in filename.lower(): return full_path, filename\n",
        "    except: return None, None\n",
        "    return None, None\n",
        "\n",
        "def execute_download(command, url, message_id):\n",
        "    process = subprocess.Popen(\" \".join(command), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', errors='replace')\n",
        "    last_edit_time = 0\n",
        "    for line in iter(process.stdout.readline, ''):\n",
        "        if '[download]' in line and '%' in line and time.time() - last_edit_time > 5:\n",
        "            progress_text = line.strip().replace(\"[download]\", \"üìä\").replace(\"ETA\", \"\\n‚è≥ ETA\")\n",
        "            edit_telegram_message(f\"üöÄ **Downloading:**\\n`{url}`\\n\\n`{progress_text}`\", message_id)\n",
        "            last_edit_time = time.time()\n",
        "    process.wait()\n",
        "    return process.returncode, process.stderr.read()\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "print(\"Step 2: Connecting to Google Drive...\");\n",
        "try: drive.mount('/content/drive', force_remount=True); print(\"‚úÖ Google Drive connected.\")\n",
        "except Exception as e: send_telegram_message(f\"üö® Bot failed: Could not connect to GDrive. Error: {e}\"); import sys; sys.exit()\n",
        "print(\"-\" * 50)\n",
        "STATE_FILE = '/content/drive/MyDrive/telegram_downloader_last_update.txt'; last_update_id = 0\n",
        "try:\n",
        "    with open(STATE_FILE, 'r') as f: last_update_id = int(f.read().strip())\n",
        "    print(f\"‚úÖ Resuming from last processed message (ID: {last_update_id})\")\n",
        "except: print(\"‚ÑπÔ∏è No state file found.\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Step 4: Fetching new messages...\"); new_messages, new_offset = [], last_update_id\n",
        "try:\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/getUpdates?offset={last_update_id + 1}&timeout=10\"\n",
        "    response = requests.get(url, timeout=15).json()\n",
        "    if response.get(\"ok\"):\n",
        "        new_messages = response.get(\"result\", []); print(f\"‚úÖ Found {len(new_messages)} new message(s).\")\n",
        "        if new_messages: new_offset = new_messages[-1]['update_id']\n",
        "    else: print(f\"üö® Telegram API error: {response.get('description')}\")\n",
        "except Exception as e: send_telegram_message(f\"üö® Bot failed: Could not connect to Telegram API. Error: {e}\"); import sys; sys.exit()\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if not new_messages:\n",
        "    send_telegram_message(\"ü§∑‚Äç‚ôÄÔ∏è I'm all caught up! No new links to download.\")\n",
        "else:\n",
        "    domains_requiring_cookies = set()\n",
        "    total_urls_found = 0\n",
        "    for update in new_messages:\n",
        "        # <-- LOGIC CHANGE IS HERE ---\n",
        "        urls_to_download = get_urls_from_update(update)\n",
        "        if not urls_to_download: continue\n",
        "        total_urls_found += len(urls_to_download)\n",
        "\n",
        "        for url in urls_to_download:\n",
        "            print(f\"\\nProcessing URL: {url}\");\n",
        "            message_id = send_telegram_message(f\"üöÄ **Starting download for:**\\n`{url}`\");\n",
        "            domain_keyword = get_domain_keyword(url)\n",
        "\n",
        "            base_command = [\"yt-dlp\", \"-P\", shlex.quote(OUTPUT_DIRECTORY), \"-o\", shlex.quote(FILENAME_TEMPLATE), \"--newline\", \"--progress\"]\n",
        "            quality_map = {\"1080p (Full HD)\": \"bestvideo[height<=1080]+bestaudio/best\", \"720p (HD)\": \"bestvideo[height<=720]+bestaudio/best\"}\n",
        "            if VIDEO_QUALITY in quality_map: base_command.extend([\"-f\", shlex.quote(quality_map[VIDEO_QUALITY])])\n",
        "            if POST_PROCESSING == \"Extract Audio (mp3)\": base_command.extend([\"-x\", \"--audio-format\", \"mp3\"])\n",
        "            if ADDITIONAL_YTDLP_FLAGS: base_command.extend(shlex.split(ADDITIONAL_YTDLP_FLAGS))\n",
        "\n",
        "            cookie_path, cookie_filename = find_cookie_file_by_keyword(domain_keyword)\n",
        "\n",
        "            def attempt_download(use_cookies=False):\n",
        "                cmd = list(base_command)\n",
        "                if use_cookies and cookie_path: cmd.extend([\"--cookies\", shlex.quote(cookie_path)])\n",
        "                cmd.append(shlex.quote(url))\n",
        "                return execute_download(cmd, url, message_id)\n",
        "\n",
        "            if domain_keyword in domains_requiring_cookies:\n",
        "                print(f\"‚ö†Ô∏è Domain '{domain_keyword}' is flagged. Attempting direct download with cookies.\")\n",
        "                if not cookie_path:\n",
        "                    edit_telegram_message(f\"üö® **Download Failed!**\\n`{url}`\\n\\n*Error:* Domain requires login, but no cookie file found.\", message_id); continue\n",
        "                returncode, stderr = attempt_download(use_cookies=True)\n",
        "            else:\n",
        "                returncode, stderr = attempt_download(use_cookies=False)\n",
        "\n",
        "            if returncode == 0:\n",
        "                edit_telegram_message(f\"‚úÖ **Download Complete!**\\n`{url}`\", message_id); continue\n",
        "\n",
        "            retry_keywords = ['sign in', 'login required', 'private', '403', '401', '429', 'log in', 'unavailable', 'members-only', 'confirm your age', 'unreachable']\n",
        "            if any(keyword in stderr.lower() for keyword in retry_keywords):\n",
        "                print(f\"Auth error detected. Flagging domain '{domain_keyword}' and retrying.\")\n",
        "                domains_requiring_cookies.add(domain_keyword)\n",
        "                if not cookie_path:\n",
        "                    edit_telegram_message(f\"üö® **Download Failed!**\\n`{url}`\\n\\n*Error:* Login required, but no cookie file found for `{domain_keyword}`.\", message_id); continue\n",
        "                returncode, stderr = attempt_download(use_cookies=True)\n",
        "\n",
        "            if returncode == 0:\n",
        "                edit_telegram_message(f\"‚úÖ **Retry Successful!**\\nDownload complete for `{url}`\", message_id)\n",
        "            else:\n",
        "                error_line = next((line for line in reversed(stderr.splitlines()) if line.strip().upper().startswith('ERROR:')), \"Unknown error.\")\n",
        "                edit_telegram_message(f\"üö® **Download Failed!**\\n`{url}`\\n\\n*Error:* `{error_line}`\", message_id)\n",
        "\n",
        "    if total_urls_found == 0:\n",
        "        # This message will now only appear if new text messages were found, but none contained URLs.\n",
        "        send_telegram_message(\"ü§∑‚Äç‚ôÄÔ∏è I checked your new messages but couldn't find any links to download.\")\n",
        "\n",
        "if new_offset > last_update_id:\n",
        "    with open(STATE_FILE, 'w') as f: f.write(str(new_offset))\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n‚úÖ All tasks complete. State saved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ptOXJMvrU8Hf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMRrYBNuXeYh5ZqxfbQwzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}